# your_package/document_processor/document_processor.py
import logging, re, bisect
from pathlib import Path
from typing import Any, Dict, List, Optional

from libs.core.functions.utils import (clean_text, clean_code_text, sanitize_text_for_json)
from libs.chunking.chunking import (split_text_preserving_html_blocks, reconstruct_text_from_chunks,
                       find_overlap_length, chunk_code_text, estimate_chunks_count)
from libs.core.processor.pdf_handler import extract_text_from_pdf
from libs.core.processor.pdf_legacy.pdf_handler_ocr import extract_text_from_pdf_ocr
from libs.core.processor.pdf_handler_v4 import extract_text_from_pdf_v4
from libs.core.processor.docx_handler import extract_text_from_docx
from libs.core.processor.doc_handler import extract_text_from_doc
from libs.core.processor.ppt_handler import extract_text_from_ppt
from libs.core.processor.excel_handler import extract_text_from_excel
from libs.core.processor.csv_handler import extract_text_from_csv
from libs.core.processor.text_handler import extract_text_from_text_file
from libs.core.processor.hwp_processor import extract_text_from_hwp
from libs.core.processor.hwpx_processor import extract_text_from_hwpx

logger = logging.getLogger("document-processor")

class DocumentProcessor:
    """
    ì›ë³¸ í´ë˜ìŠ¤ì˜ ê³µê°œ ë©”ì„œë“œ/ì‹œê·¸ë‹ˆì²˜/ë™ì‘ì„ ê·¸ëŒ€ë¡œ ìœ ì§€.
    ë‚´ë¶€ êµ¬í˜„ì€ ëª¨ë“ˆë¡œ ë¶„ë¦¬(ìœ„ì„).
    """
    def __init__(self, config_composer = None):
        self.config_composer = config_composer

        # íƒ€ì… ì„¸íŠ¸(ì›ë³¸ê³¼ ë™ì¼)
        self.document_types = ['pdf', 'docx', 'doc', 'pptx', 'ppt', 'hwp', 'hwpx']
        self.text_types = ['txt','md','markdown','rtf']
        self.code_types = ['py','js','ts','java','cpp','c','h','cs','go','rs',
                           'php','rb','swift','kt','scala','dart','r','sql',
                           'html','css','jsx','tsx','vue','svelte']
        self.config_types = ['json','yaml','yml','xml','toml','ini','cfg','conf','properties','env']
        self.data_types   = ['csv','tsv','xlsx','xls']
        self.script_types = ['sh','bat','ps1','zsh','fish']
        self.log_types    = ['log']
        self.web_types    = ['htm','xhtml']
        self.image_types  = ['jpg','jpeg','png','gif','bmp','webp']

        self.supported_types = ( self.document_types + self.text_types + self.code_types +
                                 self.config_types + self.data_types + self.script_types +
                                 self.log_types + self.web_types + self.image_types )

        # ê°€ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬(ì›ë³¸ê³¼ ë™ì¼í•œ ê²½ê³ /ë™ì‘)
        try:
            from openpyxl import load_workbook  # noqa
            OPENPYXL_AVAILABLE = True
        except Exception:
            OPENPYXL_AVAILABLE = False
        try:
            import xlrd  # noqa
            XLRD_AVAILABLE = True
        except Exception:
            XLRD_AVAILABLE = False
        try:
            from langchain_openai import ChatOpenAI  # noqa
            LANGCHAIN_OPENAI_AVAILABLE = True
        except Exception:
            LANGCHAIN_OPENAI_AVAILABLE = False
        try:
            from pdfminer.high_level import extract_text  # noqa
            PDFMINER_AVAILABLE = True
        except Exception:
            PDFMINER_AVAILABLE = False
        try:
            from pdf2image import convert_from_path  # noqa
            PDF2IMAGE_AVAILABLE = True
        except Exception:
            PDF2IMAGE_AVAILABLE = False
        try:
            from docx2pdf import convert as docx_to_pdf_convert  # noqa
            DOCX2PDF_AVAILABLE = True
        except Exception:
            DOCX2PDF_AVAILABLE = False
        try:
            from pptx import Presentation  # noqa
            PYTHON_PPTX_AVAILABLE = True
        except Exception:
            PYTHON_PPTX_AVAILABLE = False
        try:
            from PIL import Image  # noqa
            PIL_AVAILABLE = True
        except Exception:
            PIL_AVAILABLE = False

        if not OPENPYXL_AVAILABLE and not XLRD_AVAILABLE:
            self.supported_types = [t for t in self.supported_types if t not in ['xlsx','xls']]
            logger.warning("openpyxl and xlrd not available. Excel processing disabled.")
        if not LANGCHAIN_OPENAI_AVAILABLE:
            self.supported_types = [t for t in self.supported_types if t not in self.image_types]
            logger.warning("langchain_openai not available. Image processing disabled.")
        if not PDFMINER_AVAILABLE:
            logger.warning("pdfminer not available. Using PyPDF2 fallback.")
        if not PDF2IMAGE_AVAILABLE:
            logger.warning("pdf2image not available. OCR disabled.")
        if not DOCX2PDF_AVAILABLE and not PIL_AVAILABLE:
            logger.warning("docx2pdf and PIL not available. DOCX/PPT OCR disabled.")

        self.encodings = ['utf-8','utf-8-sig','cp949','euc-kr','latin-1','ascii']

    # === ê¸°ì¡´ private ë©”ì„œë“œ ëŒ€ì²´ ===
    def _get_current_image_text_config(self) -> Dict[str, Any]:
        if self.config_composer:
            provider = str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower()

            if provider == "openai":
                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'base_url': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_BASE_URL").value),
                    'api_key': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_API_KEY").value),
                    'model': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_MODEL_NAME").value),
                    'temperature': float(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_TEMPERATURE").value),
                    'image_quality': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_IMAGE_QUALITY").value).lower(),
                    'batch_size': int(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_BATCH_SIZE").value),
                }

            elif provider == "anthropic":
                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'base_url': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_BASE_URL").value),
                    'api_key': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_API_KEY").value),
                    'model': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_MODEL_NAME").value),
                    'temperature': float(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_TEMPERATURE").value),
                    'image_quality': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_IMAGE_QUALITY").value).lower(),
                    'batch_size': int(self.config_composer.get_config_by_name("VISION_LANGUAGE_ANTHROPIC_BATCH_SIZE").value),
                }

            elif provider == "gemini":
                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'base_url': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_BASE_URL").value),
                    'api_key': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_API_KEY").value),
                    'model': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_MODEL_NAME").value),
                    'temperature': float(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_TEMPERATURE").value),
                    'image_quality': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_IMAGE_QUALITY").value).lower(),
                    'batch_size': int(self.config_composer.get_config_by_name("VISION_LANGUAGE_GEMINI_BATCH_SIZE").value),
                }

            elif provider == "vllm":
                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'base_url': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_VLLM_BASE_URL").value),
                    'api_key': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_VLLM_API_KEY").value),
                    'model': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_VLLM_MODEL_NAME").value),
                    'temperature': float(self.config_composer.get_config_by_name("VISION_LANGUAGE_OPENAI_TEMPERATURE").value),
                    'image_quality': 'auto',
                    'batch_size': 1,
                }

            elif provider == "aws_bedrock":
                # AWS Bedrock endpoint_url ì²˜ë¦¬
                endpoint_url_raw = str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_BEDROCK_ENDPOINT_URL").value)
                endpoint_url = None if endpoint_url_raw.lower() in ("", "auto", "none", "unset") else endpoint_url_raw

                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'aws_access_key_id': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_ACCESS_KEY_ID").value),
                    'aws_secret_access_key': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_SECRET_ACCESS_KEY").value),
                    'aws_session_token': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_SESSION_TOKEN").value),
                    'aws_region': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_REGION").value),
                    'endpoint_url': endpoint_url,
                    'model': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_AWS_MODEL_NAME").value),
                    'temperature': 0.0,
                    'image_quality': 'auto',
                    'batch_size': 1,
                }

            elif provider == "no_model":
                config = {
                    'provider': str(self.config_composer.get_config_by_name("VISION_LANGUAGE_MODEL_PROVIDER").value).lower(),
                    'base_url': "",
                    'api_key': "",
                    'model': "",
                    'temperature': 0,
                    'image_quality': 'auto',
                    'batch_size': 1,
                }

            else:
                raise ValueError(f"Unsupported VISION_LANGUAGE_MODEL_PROVIDER: {provider}")
        else:
            raise ValueError("Config composer not provided")

        return config

    # def _is_image_text_enabled(self, config: Dict[str, Any]) -> bool:
    #     try:
    #         from langchain_openai import ChatOpenAI  # noqa
    #         langchain_ok = True
    #     except Exception:
    #         langchain_ok = False
    #     return is_image_text_enabled(config, langchain_ok)

    # === ê³µê°œ APIë“¤: ì›ë³¸ê³¼ ë™ì¼ ì‹œê·¸ë‹ˆì²˜/ë™ì‘ ===

    def get_supported_types(self) -> List[str]:
        return self.supported_types.copy()

    def get_file_category(self, file_type: str) -> str:
        ft = file_type.lower()
        if ft in self.document_types: return 'document'
        if ft in self.text_types:     return 'text'
        if ft in self.code_types:     return 'code'
        if ft in self.config_types:   return 'config'
        if ft in self.data_types:     return 'data'
        if ft in self.script_types:   return 'script'
        if ft in self.log_types:      return 'log'
        if ft in self.web_types:      return 'web'
        if ft in self.image_types:    return 'image'
        return 'unknown'

    def clean_text(self, text: str) -> str:
        return clean_text(text)

    def _is_text_quality_sufficient(self, text: Optional[str], min_chars: int = 500, min_word_ratio: float = 0.6) -> bool:
        from libs.core.functions.utils import is_text_quality_sufficient
        return is_text_quality_sufficient(text, min_chars, min_word_ratio)

    def clean_code_text(self, text: str, file_type: str) -> str:
        return clean_code_text(text)

    async def extract_text_from_file(
        self,
        file_path: str,
        file_extension: str,
        process_type: str,
        app_db=None,
        extract_default_metadata: bool = True,
        minio_bucket: Optional[str] = None,
        minio_object_name: Optional[str] = None,
    ) -> str:
        """
        íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            file_extension: íŒŒì¼ í™•ì¥ì
            process_type: ì²˜ë¦¬ ìœ í˜•
            app_db: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            extract_default_metadata: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                   - True: ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(ì œëª©, ì‘ì„±ì, ìƒì„±ì¼ ë“±)ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²°ê³¼ì— í¬í•¨
                   - False: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ìƒëµ
            minio_bucket: MinIO ë²„í‚· ì´ë¦„ (ì„ íƒì‚¬í•­, ë¡œê¹…/ì¶”ì ìš©)
            minio_object_name: MinIO ê°ì²´ ì´ë¦„ (ì„ íƒì‚¬í•­, ë¡œê¹…/ì¶”ì ìš©)

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        category = self.get_file_category(file_extension)
        minio_info = f" (MinIO: {minio_bucket}/{minio_object_name})" if minio_bucket and minio_object_name else ""
        logger.info(f"Extracting text from {file_extension} file ({category} category): {file_path}{minio_info}")

        # configê°€ í•„ìš”í•œ íŒŒì¼ íƒ€ì…ì—ë§Œ configë¥¼ ê°€ì ¸ì˜´
        cfg = None
        try:
            cfg = self._get_current_image_text_config()
        except ValueError:
            # Excel/CSV/DOC ë“± config ì—†ì´ë„ ë™ì‘í•˜ëŠ” í•¸ë“¤ëŸ¬
            if file_extension not in ['xlsx', 'xls', 'csv', 'tsv', 'doc', 'docx', 'pptx', 'ppt', 'hwp', 'hwpx']:
                raise

        # ì¶”ì¶œëœ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
        result: str = ""

        if file_extension == 'pdf':
            # process_typeì— ë”°ë¥¸ PDF ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
            if process_type == "default":
                # PyMuPDF ì „ìš© ê³ ì† ì²˜ë¦¬ (pdfplumber ë¯¸ì‚¬ìš©, ê°•ê±´í•œ í…Œì´ë¸” ì¶”ì¶œ)
                # V4ì™€ ë™ì¼ ì²˜ë¦¬ë¡œ í†µí•©(ì¶”í›„ ì œê±°)
                result = await extract_text_from_pdf_v4(file_path, cfg, app_db, extract_default_metadata)
            elif process_type == "enhanced":
                # ì´ë¯¸ì§€/í…Œì´ë¸” ì¶”ì¶œ í¬í•¨ (pdfplumber ìš°ì„ )
                result = await extract_text_from_pdf(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            elif process_type == "enhanced_v2":
                # ê³ ë„í™”ëœ í…Œì´ë¸” ì¶”ì¶œ V2 (PyMuPDF find_tables ê¸°ë°˜, ë³µì¡í•œ ë³‘í•© ì…€ ì²˜ë¦¬)
                # V4ì™€ ë™ì¼ ì²˜ë¦¬ë¡œ í†µí•©(ì¶”í›„ ì œê±°)
                result = await extract_text_from_pdf_v4(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            elif process_type == "enhanced_v3":
                # ê³ ë„í™”ëœ í…Œì´ë¸” ì¶”ì¶œ V3 (ë‹¤ì¤‘ ì „ëµ í…Œì´ë¸” ê°ì§€, ë²¡í„° í…ìŠ¤íŠ¸ OCR, ë³µì¡í•œ ë³‘í•© ì…€)
                # V4ì™€ ë™ì¼ ì²˜ë¦¬ë¡œ í†µí•©(ì¶”í›„ ì œê±°)
                result = await extract_text_from_pdf_v4(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            elif process_type == "enhanced_v4":
                # ì ì‘í˜• ë³µì¡ë„ ê¸°ë°˜ ì²˜ë¦¬ V4 (ë³µì¡í•œ ì˜ì—­ ì´ë¯¸ì§€í™” + MinIO ì—…ë¡œë“œ)
                result = await extract_text_from_pdf_v4(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            elif process_type == "enhanced_unstructured":
                # pdfminer.six + unstructured cleaning ê¸°ë°˜ ì²˜ë¦¬
                # V4ì™€ ë™ì¼ ì²˜ë¦¬ë¡œ í†µí•© (ì¶”í›„ ì œê±°)
                result = await extract_text_from_pdf_v4(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            elif process_type == "enhanced_ocr":
                # ê³ ë„í™”ëœ OCR ê¸°ë°˜ ì²˜ë¦¬: í…ìŠ¤íŠ¸ëŠ” í…ìŠ¤íŠ¸ë¡œ, ë¹„ì •í˜•ì€ ì´ë¯¸ì§€ë¡œ
                result = await extract_text_from_pdf_ocr(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
            else:
                # ê¸°ë³¸ê°’
                result = await extract_text_from_pdf(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension == 'docx':
            # DOCX enhanced ëª¨ë“œ ì§€ì› (ê¸°ë³¸ê°’)
            result = await extract_text_from_docx(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension == 'doc':
            # êµ¬í˜• DOC íŒŒì¼ ì²˜ë¦¬ (RTF, OLE, HTML ìë™ ê°ì§€)
            result = await extract_text_from_doc(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension in ['pptx', 'ppt']:
            # PPTë„ enhanced ëª¨ë“œ ì§€ì› (ê¸°ë³¸ê°’)
            result = await extract_text_from_ppt(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension in ['xlsx', 'xls']:
            # Excel íŒŒì¼ enhanced ëª¨ë“œ ì§€ì› (ì°¨íŠ¸, ì´ë¯¸ì§€, ë³‘í•© ì…€)
            result = await extract_text_from_excel(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension in ['csv','tsv']:
            # CSV/TSV enhanced ëª¨ë“œ ì§€ì› (HTML í…Œì´ë¸”, ìë™ ì¸ì½”ë”©/êµ¬ë¶„ì ê°ì§€)
            result = await extract_text_from_csv(file_path, cfg, extract_default_metadata=extract_default_metadata)
        elif file_extension == 'hwp':
            result = await extract_text_from_hwp(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension == 'hwpx':
            result = await extract_text_from_hwpx(file_path, cfg, app_db, extract_default_metadata=extract_default_metadata)
        elif file_extension in self.image_types:
            # ì´ë¯¸ì§€ íŒŒì¼ì€ ë‹¨ê±´ OCR(ì›ë³¸ê³¼ ë™ì¼ ë™ì‘: ë‚´ë¶€ì—ì„œ config ê²€ì‚¬)
            from libs.core.ocr_legacy import convert_image_to_text
            result = await convert_image_to_text(file_path, cfg)
        elif file_extension in (self.text_types + self.code_types + self.config_types +
                                self.script_types + self.log_types + self.web_types):
            is_code = file_extension in self.code_types
            result = await extract_text_from_text_file(file_path, file_extension, self.encodings, is_code=is_code)
        else:
            raise ValueError(f"Unsupported file type: {file_extension}")

        # ëª¨ë“  ê²°ê³¼ì— ëŒ€í•´ UTF-8 JSON ì¸ì½”ë”© ì•ˆì „ì„± ë³´ì¥
        # ì„œë¡œê²Œì´íŠ¸ ë¬¸ì, PUA ë¬¸ì, ë¹„ë¬¸ì ë“± ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë¬¸ì ì œê±°
        return sanitize_text_for_json(result)

    # === ì²­í‚¹ ê´€ë ¨ ===
    def chunk_text(
        self,
        text: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        file_extension: Optional[str] = None,
        force_chunking: Optional[bool] = False,
        # Advanced Settings (í–¥í›„ êµ¬í˜„ ì˜ˆì •, í˜„ì¬ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ì¸ìë§Œ ìˆ˜ìš©)
        chunking_strategy: str = "recursive",
        stride: Optional[int] = None,
        parent_chunk_size: Optional[int] = None,
        child_chunk_size: Optional[int] = None,
        **kwargs  # ì¶”ê°€ ì¸ì ë¬´ì‹œ
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
            file_extension: íŒŒì¼ í™•ì¥ì (csv, xlsx ë“±) - í…Œì´ë¸” ê¸°ë°˜ ì²˜ë¦¬ ê²°ì •ì— ì‚¬ìš©
            force_chunking: ê°•ì œ ì²­í‚¹ ì—¬ë¶€ (í…Œì´ë¸” ê¸°ë°˜ íŒŒì¼ ì œì™¸)
            chunking_strategy: ì²­í‚¹ ì „ëµ (recursive, sliding, hierarchical) - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            stride: Sliding Window ì „ëµì—ì„œì˜ ìŠ¤íŠ¸ë¼ì´ë“œ - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            parent_chunk_size: Hierarchical ì „ëµì—ì„œì˜ ë¶€ëª¨ ì²­í¬ í¬ê¸° - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            child_chunk_size: Hierarchical ì „ëµì—ì„œì˜ ìì‹ ì²­í¬ í¬ê¸° - í–¥í›„ êµ¬í˜„ ì˜ˆì •

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # TODO: chunking_strategyì— ë”°ë¥¸ ë‹¤ì–‘í•œ ì²­í‚¹ ì „ëµ êµ¬í˜„
        # í˜„ì¬ëŠ” ê¸°ë³¸ recursive ì „ëµë§Œ ì‚¬ìš©
        if chunking_strategy != "recursive":
            logger.warning(f"Chunking strategy '{chunking_strategy}' is not yet implemented, falling back to 'recursive'")

        return split_text_preserving_html_blocks(text, chunk_size, chunk_overlap, file_extension, force_chunking)

    def _reconstruct_text_from_chunks(self, chunks: List[str], chunk_overlap: int) -> str:
        return reconstruct_text_from_chunks(chunks, chunk_overlap)

    def _find_overlap_length(self, chunk1: str, chunk2: str, max_overlap: int) -> int:
        return find_overlap_length(chunk1, chunk2, max_overlap)

    def chunk_code_text(self, text: str, file_type: str, chunk_size: int = 1500, chunk_overlap: int = 300) -> List[str]:
        return chunk_code_text(text, file_type, chunk_size, chunk_overlap)

    def estimate_chunks_count(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> int:
        return estimate_chunks_count(text, chunk_size, chunk_overlap)

    # === ë¼ì¸/í˜ì´ì§€ ë§¤í•‘(ì›ë³¸ ë¡œì§ê³¼ ë™ì¼ íš¨ê³¼) ===
    def _extract_page_mapping(self, text: str, file_extension: str) -> List[Dict[str, Any]]:
        try:
            page_mapping: List[Dict[str, Any]] = []
            if file_extension in ['pdf','ppt','pptx','doc','docx']:
                patterns = [
                    r'=== í˜ì´ì§€ (\d+) ===',
                    r'=== í˜ì´ì§€ (\d+) \(OCR\) ===',
                    r'=== í˜ì´ì§€ (\d+) \(OCR\+ì°¸ê³ \) ===',
                    r'=== ìŠ¬ë¼ì´ë“œ (\d+) ===',
                    r'=== ìŠ¬ë¼ì´ë“œ (\d+) \(OCR\) ===',
                    r'<í˜ì´ì§€\s*ë²ˆí˜¸>\s*(\d+)\s*</í˜ì´ì§€\s*ë²ˆí˜¸>',
                    r'<í˜ì´ì§€\s*ë²ˆí˜¸>\s*(\d+)\s*\(OCR\)\s*</í˜ì´ì§€\s*ë²ˆí˜¸>',
                    r'<í˜ì´ì§€\s*ë²ˆí˜¸>\s*(\d+)\s*\(OCR\+ì°¸ê³ \)\s*</í˜ì´ì§€\s*ë²ˆí˜¸>',
                    r'<ìŠ¬ë¼ì´ë“œ\s*ë²ˆí˜¸>\s*(\d+)\s*</ìŠ¬ë¼ì´ë“œ\s*ë²ˆí˜¸>',
                    r'<ìŠ¬ë¼ì´ë“œ\s*ë²ˆí˜¸>\s*(\d+)\s*\(OCR\)\s*</ìŠ¬ë¼ì´ë“œ\s*ë²ˆí˜¸>',
                ]
                for pat in patterns:
                    matches = list(re.finditer(pat, text))
                    if matches:
                        for i, m in enumerate(matches):
                            page_num = int(m.group(1))
                            start = m.end()
                            end = matches[i+1].start() if i+1 < len(matches) else len(text)
                            page_mapping.append({"page_num": page_num, "start_pos": start, "end_pos": end})
                        page_mapping.sort(key=lambda x: x["page_num"])
                        break
                if not page_mapping and file_extension in ['doc','docx']:
                    chars_per_page = 1500
                    L = len(text)
                    if L > chars_per_page:
                        est = (L + chars_per_page - 1)//chars_per_page
                        for pn in range(1, est+1):
                            s = (pn-1)*chars_per_page
                            e = min(pn*chars_per_page, L)
                            page_mapping.append({"page_num": pn, "start_pos": s, "end_pos": e})
                if not page_mapping:
                    page_mapping = [{"page_num":1,"start_pos":0,"end_pos":len(text)}]
            elif file_extension in ['xlsx','xls']:
                matches = list(re.finditer(r'=== ì‹œíŠ¸: ([^=]+) ===', text))
                if matches:
                    for i, m in enumerate(matches):
                        s = m.end()
                        e = matches[i+1].start() if i+1 < len(matches) else len(text)
                        page_mapping.append({"page_num": i+1, "start_pos": s, "end_pos": e,
                                             "sheet_name": m.group(1).strip()})
                else:
                    page_mapping = [{"page_num":1,"start_pos":0,"end_pos":len(text)}]
            else:
                lines = text.split('\n')
                lpp = 1000
                if len(lines) > lpp:
                    pc = (len(lines) + lpp - 1)//lpp
                    cur = 0
                    for pn in range(1, pc+1):
                        sline = (pn-1)*lpp
                        eline = min(pn*lpp, len(lines))
                        page_text = '\n'.join(lines[sline:eline])
                        s = cur; e = cur + len(page_text)
                        page_mapping.append({"page_num": pn, "start_pos": s, "end_pos": e})
                        cur = e + 1
                else:
                    page_mapping = [{"page_num":1,"start_pos":0,"end_pos":len(text)}]
            return page_mapping
        except Exception:
            return [{"page_num":1,"start_pos":0,"end_pos":len(text)}]

    def _find_line_index_by_pos(self, pos: int, line_table: List[Dict[str, int]]) -> int:
        try:
            if not line_table:
                return 0
            starts = [l["start"] for l in line_table]
            idx = bisect.bisect_right(starts, pos) - 1
            return 0 if idx < 0 else min(idx, len(line_table)-1)
        except Exception:
            return 0

    def _build_line_offset_table(self, text: str, file_extension: str) -> List[Dict[str, int]]:
        try:
            lines = text.split('\n')
            table: List[Dict[str, int]] = []
            pos = 0
            page_mapping = self._extract_page_mapping(text, file_extension)
            def _page_for_pos(p: int) -> int:
                for info in page_mapping:
                    if info["start_pos"] <= p < info["end_pos"]:
                        return info["page_num"]
                return 1
            for i, line in enumerate(lines):
                start = pos
                end = pos + len(line)
                mid = start + max(0, (end-start)//2)
                page = _page_for_pos(mid)
                table.append({"line_num": i+1, "start": start, "end": end, "page": page})
                pos = end + 1
            return table
        except Exception:
            return [{"line_num":1,"start":0,"end":len(text),"page":1}]

    def chunk_text_with_metadata(
        self,
        text: str,
        file_extension: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        force_chunking: Optional[bool] = False,
        # Advanced Settings (í–¥í›„ êµ¬í˜„ ì˜ˆì •, í˜„ì¬ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ì¸ìë§Œ ìˆ˜ìš©)
        chunking_strategy: str = "recursive",
        stride: Optional[int] = None,
        parent_chunk_size: Optional[int] = None,
        child_chunk_size: Optional[int] = None,
        **kwargs  # ì¶”ê°€ ì¸ì ë¬´ì‹œ
    ) -> List[Dict[str, Any]]:
        """
        ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            file_extension: íŒŒì¼ í™•ì¥ì
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
            force_chunking: ê°•ì œ ì²­í‚¹ ì—¬ë¶€ (í…Œì´ë¸” ê¸°ë°˜ íŒŒì¼ ì œì™¸)
            chunking_strategy: ì²­í‚¹ ì „ëµ (recursive, sliding, hierarchical) - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            stride: Sliding Window ì „ëµì—ì„œì˜ ìŠ¤íŠ¸ë¼ì´ë“œ - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            parent_chunk_size: Hierarchical ì „ëµì—ì„œì˜ ë¶€ëª¨ ì²­í¬ í¬ê¸° - í–¥í›„ êµ¬í˜„ ì˜ˆì •
            child_chunk_size: Hierarchical ì „ëµì—ì„œì˜ ìì‹ ì²­í¬ í¬ê¸° - í–¥í›„ êµ¬í˜„ ì˜ˆì •

        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # file_extensionì„ chunk_textì— ì „ë‹¬í•˜ì—¬ CSV/Excel ë“± í…Œì´ë¸” ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬
        chunks = self.chunk_text(
            text, chunk_size, chunk_overlap,
            file_extension=file_extension,
            force_chunking=force_chunking,
            chunking_strategy=chunking_strategy,
            stride=stride,
            parent_chunk_size=parent_chunk_size,
            child_chunk_size=child_chunk_size
        )
        reconstructed = self._reconstruct_text_from_chunks(chunks, chunk_overlap)
        line_table = self._build_line_offset_table(reconstructed, file_extension)

        out: List[Dict[str, Any]] = []
        cur = 0
        for idx, ch in enumerate(chunks):
            start = cur
            end = cur + len(ch) - 1
            sidx = self._find_line_index_by_pos(start, line_table)
            eidx = self._find_line_index_by_pos(end, line_table)
            line_start = line_table[sidx]["line_num"]
            line_end = line_table[eidx]["line_num"]
            page_number = line_table[sidx].get("page", 1)
            out.append({
                "text": ch,
                "page_number": page_number,
                "line_start": line_start,
                "line_end": line_end,
                "global_start": start,
                "global_end": end,
                "chunk_index": idx
            })
            cur += len(ch)
            if idx < len(chunks) - 1:
                ov = find_overlap_length(ch, chunks[idx+1], chunk_overlap)
                cur -= ov
        logger.info(f"Created {len(out)} chunks with metadata using reconstructed text")
        return out

    def validate_file_format(self, file_path: str) -> tuple[bool, str]:
        try:
            ext = Path(file_path).suffix[1:].lower()
            return (ext in self.supported_types, ext)
        except Exception:
            return (False, "")

    def get_file_info(self, file_path: str) -> Dict[str, str]:
        try:
            ext = Path(file_path).suffix[1:].lower()
            cat = self.get_file_category(ext)
            ok = ext in self.supported_types
            return {'extension': ext, 'category': cat, 'supported': str(ok)}
        except Exception:
            return {'extension':'unknown','category':'unknown','supported':'false'}

    async def extract_text_from_repository(
        self,
        gitlab_url: str,
        gitlab_token: str,
        repository_path: str,
        branch: str = "main",
        enable_annotation: bool = False,
        enable_api_extraction: bool = False,
        progress_callback = None
    ) -> Dict[str, Any]:
        """
        GitLab ë ˆí¬ì§€í† ë¦¬ì—ì„œ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜

        Args:
            gitlab_url: GitLab ì¸ìŠ¤í„´ìŠ¤ URL
            gitlab_token: Personal Access Token
            repository_path: ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ (ì˜ˆ: group/project)
            branch: ë¸Œëœì¹˜ ì´ë¦„
            enable_annotation: LLM ê¸°ë°˜ ì½”ë“œ ì£¼ì„ ìƒì„± ì—¬ë¶€
            enable_api_extraction: API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (Optional[Callable])

        Returns:
            {
                'files': [{
                    'path': str,
                    'content': str,
                    'annotated_content': str (optional),
                    'language': str,
                    'size': int
                }],
                'api_info': [...] (optional),
                'metadata': {...}
            }
        """
        cfg = self._get_current_image_text_config()

        return await extract_text_from_code_repository(
            gitlab_url=gitlab_url,
            gitlab_token=gitlab_token,
            repository_path=repository_path,
            branch=branch,
            config=cfg,
            enable_annotation=enable_annotation,
            enable_api_extraction=enable_api_extraction,
            progress_callback=progress_callback
        )

    def test(self):
        try:
            cfg = self._get_current_image_text_config()
            logger.info(f"ğŸ” Test - Current provider: {cfg.get('provider','no_model')}")
            logger.info(f"ğŸ” Test - Current config: {cfg}")
            try:
                from langchain_openai import ChatOpenAI  # noqa
                langchain_ok = True
            except Exception:
                langchain_ok = False
        except Exception as e:
            logger.error(f"Error in test method: {e}")
            raise
